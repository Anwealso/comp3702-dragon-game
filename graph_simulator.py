import sys
import platform
import logging
import time
import random
import hashlib
from game_env import GameEnv
from graph_solution import RLAgent
import matplotlib.pyplot as plt


# automatic timeout handling will only be performed on Unix
if platform.system() != 'Windows':
    import signal
    WINDOWS = False
else:
    WINDOWS = True

DEBUG_MODE = False  # set to True to disable time limit checks

CRASH = 255
OVERTIME = 254

"""
Simulator script.

Run this file to evaluate the performance of the policy generated by your solver for a given input file. You may modify
this file if desired. When submitting to GradeScope, an unmodified version of this file will be used to evaluate your
code.

The return code produced by simulator is your agent's score for the testcase (multiplied by 10 and represented as an
integer).

Simulator seeds random outcomes to produce consistent policy performance between runs - if your code is deterministic
and does not exceed the time limit, simulator will always produce the a consistent score.

The simulator will automatically terminate your agent if it runs over 2x the allowed time limit for any step (on Unix
platforms only - not available on Windows). This feature can be disabled for debugging purposes by setting
DEBUG_MODE = True above.

COMP3702 2021 Assignment 3 Support Code

Last updated by njc 25/10/21
"""


class TimeOutException(Exception):
    pass


def timeout_handler(signum, frame):
    raise TimeOutException


def stable_hash(x):
    return hashlib.md5(str(x).encode('utf-8')).hexdigest()


def main(arglist):
    if len(arglist) != 1:
        print("Running this file tests executes your code and evaluates the performance of the generated policy for the"
              " given input file.")
        print("Usage: simulator.py [input_filename]")
        return

    input_file = arglist[0]
    if '/' in input_file:
        separator = '/'
    else:
        separator = '\\'
    init_seed = stable_hash(input_file.split(separator)[-1])
    random.seed(init_seed)
    env = GameEnv(input_file)

    # initialise RL Agent
    if not WINDOWS and not DEBUG_MODE:
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(1)
    try:
        agent = RLAgent(env)
        if not WINDOWS and not DEBUG_MODE:
            signal.alarm(0)
    except TimeOutException as e:
        logging.exception(e)
        print("/!\\ Terminated due to running over 2x time limit in solver.__init__()")
        sys.exit(OVERTIME)
    except Exception as e:
        logging.exception(e)
        print("/!\\ Terminated due to exception generated in solver.__init__()")
        sys.exit(CRASH)

    # run training
    if not WINDOWS and not DEBUG_MODE:
        signal.signal(signal.SIGALRM, timeout_handler)
        signal.alarm(int(env.training_time + 1))

    path_values = []
    num_batches = 200
    batch_size = 50
    for i in range(0, num_batches):
        try:
            t0 = time.time()
            agent.run_training(batch_size)
            t_training = time.time() - t0
            if not WINDOWS and not DEBUG_MODE:
                signal.alarm(0)
        except TimeOutException as e:
            logging.exception(e)
            print("/!\\ Terminated due to running over 2x time limit in plan_offline()")
            sys.exit(OVERTIME)
        except Exception as e:
            logging.exception(e)
            print("/!\\ Terminated due to exception generated in plan_offline()")
            sys.exit(CRASH)
        training_reward = env.get_total_reward()

        # simulate episode
        t_eval_max = 0
        terminal = False
        reward = None
        eval_reward = 0
        persistent_state = env.get_init_state()
        visit_count = {persistent_state.deepcopy(): 1}
        while not terminal and eval_reward > (env.eval_reward_tgt * 2):
            # query agent to select an action
            if not WINDOWS and not DEBUG_MODE:
                signal.signal(signal.SIGALRM, timeout_handler)
                signal.alarm(int(1))
            try:
                t0 = time.time()
                action = agent.select_action(persistent_state)
                t_online = time.time() - t0
                if not WINDOWS and not DEBUG_MODE:
                    signal.alarm(0)
                if t_online > t_eval_max:
                    t_eval_max = t_online
            except TimeOutException as e:
                logging.exception(e)
                print("/!\\ Terminated due to running over 2x time limit in select_action()")
                sys.exit(OVERTIME)
            except Exception as e:
                logging.exception(e)
                print("/!\\ Terminated due to exception generated in select_action()")
                sys.exit(CRASH)

            if action not in GameEnv.ACTIONS:
                print("/!\\ Unrecognised action selected by select_action()")
                sys.exit(CRASH)

            # simulate outcome of action
            seed = (init_seed + stable_hash(str((persistent_state.row, persistent_state.col, persistent_state.gem_status)))
                    + stable_hash(visit_count[persistent_state]))
            valid, reward, persistent_state, terminal = env.perform_action(persistent_state, action, seed=seed)
            if not valid:
                print("/!\\ Invalid action selected by select_action()")
                sys.exit(CRASH)
            # updated visited state count (for de-randomisation)
            ps = persistent_state.deepcopy()
            if ps in visit_count.keys():
                visit_count[ps] += 1
            else:
                visit_count[ps] = 1
            # update episode reward
            eval_reward += reward

        print(f"Level completed with a total rewards:\n\ttraining: {round(training_reward, 1)}"
              f"\n\tevaluation: {round(eval_reward, 1)}")
        path_values.append(eval_reward)

        # _ = input("Press enter for next batch")

    xvals = []
    for i in range(0, num_batches):
        xvals.append(i*100)

    for i in range(0, len(path_values)):
        if path_values[i] < -100:
            path_values[i] = -50

    print("")
    print("======================================================")
    print(path_values)
    print("======================================================")
    print("")

    plt.plot(xvals, path_values)
    plt.show()


if __name__ == '__main__':
    main(sys.argv[1:])


